{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283b93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from bpe_tokenizer import BPECharacterTokenizer, BPEByteTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d41600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c86878ba6745a3994b716457b6be2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lauritz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lauritz\\.cache\\huggingface\\hub\\datasets--ag_news. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98236de0e53f463193b7a5ca6891cf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c5509f3b424278bef752b23d2eb610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73306f1e69b4669a5ff5fde768aa051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba2e2f2441d40888d777a06eb995806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"ag_news\")\n",
    "\n",
    "# create a tokenizer training dataset\n",
    "tokenizer_train = list(dataset[\"train\"][\"text\"][:100])\n",
    "\n",
    "char_tokenizer = BPECharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87e2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer.train(tokenizer_train, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b255c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "['<unk>', '<pad>', '<bos>', '<eos>', ' ', '!', '\"', '#', '$', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'e ', 's ', 'in', ' t', 'er', 'an', 'd ', 't ', 'on', ' th', 'or', 'ar', 'y ', 'ing', 'en', ' the ', ', ', 'l ', 'es', 'at', 'o ', 'it', 'ro', 'a ', 'ter', 're', 'ic', 'ou', 'of', 'ing ', 'es ', 'om', 'al', 'th', 'st', 'lo', 'ed ', 'ec', 'ion', 'and ', 'il', '. ', ' to ', 'el', 'ac', 'in ', 'ur', 'k ', \"'s \", '- ', 'for', 'et', 'al ', 'h ', 'er ', 'an ', 'is', 'of ', 'ha', 'uter', 'Re', 'ew', 'us', 'as', 'ear', ') ', 'Reuter', 'pl', 'ay', 'ed', 'id', 'on ', 'com', 'is ', 's, ', 'ut', 'ion ', 'n ', 'ig', 'ent', 'Th', 'at ', 'ow', 'm ', 'sh', 'are ', 'ap', 'for ', 'ai', 'ad', 'ex', 've ', 'ay ', 'In', 'uc', 'as ', 'pro', 'ol', 'un', 'en ', 'oo', 'pr', 'ew ', 'to ', 'p ', 'ers ', 'wh', 'ch', 'em', 'The ', 'ag', 'im', 'oc', 'um', 'the ', 'pan', '(Reuter', '(Reuters', '(Reuters) ', 'os', 'ent ', 'ab', 'ect', 'ul', 'ir', 'si', 'ly ', 'ey ', 'ell', 'le ', 'con', 'in the ', 'ation', 'le', 'am', 'St', 'ver', 'ud', 'and', 'it ', 'wit', 's - ', 'ee', 'qu', 'day', 'A ', 'est', 'str', 'of the ', 's.', 'igh', 'compan', 'li', 'per', '(Reuters) Reuter', '(Reuters) Reuters - ', 'ark', 'wor', 'th ', 'enc', 'ld ', 'off', 'by ', 'oun', 'cl', 'ess ', 'br', 'ks ', 'ci', 'low', 'aid ', 'cor', 'AP', 'ter ', '00', 'all ', 'ak', 'il ', 'econ', 'ies ', 'new ', 'ell ', 'est ', 'e s', 'ug', ' that ', 'ev', 'has ', 'ation ', 'all', 'mark', 'ts ', 'fro', 'ould ', 'op', 'ic ', 'bl', 'no', 'row', 'our', 'with ', 'bo', 'ore ', 'sp', 'out', 'ing the ', 'ip', 'af', 'ill', 'e, ', 'res', 'ad ', ': ', 'its ', 'over', 'et ', 'por', 'be', ' - ', 'ive ', ' b', 'or ', 'es, ', 'Goo', 'Goog', 'be ', 'ot', 'e.', 'ill ', 'vic', 'iz', 'loo', 'pric', 'econom', 's are ', ' the', 'from ', 'uct', 'ut ', 'ear ', 'ast ', 'ing a ', 'log', 'ing to ', 'we', 'e of ', 'ech', 'ity ', 'ong', 'to', 'ate ', 'up', 'ef', 'cr', 'ang', 'mon', 'rel', 'Inc', 'und', 'plan', 'des', 'out ', ' s', '-- ', 'ere ', '\\\\$', 'arg', 'up ', 'at the ', 'gr', 'ervic', 'ind', 'ult', 'yn', 'aga', 'ace ', 'iv', 'fir', 'par', \"' \", 'show', 'ial ', 'said ', 'pos', 'Fr', 'ail ', ', the ', 'dis', 'ed to ', 'ail', '  ', 'C ', 'can ', 's S', 'ian', \"y's \", 'our ', 'ack', 'one ', 'ile ', 'ob', 'ist', 'ome ', 'echno', 'echnolog', 'proj', '. B', 'aw', 'ard ', 'mer', 'oth', 'exp', ' to', 'wee', 'ine ', 'oil ', 'US', 's and ', 'for the ', 'sur', 'ov', ', and ', 'his ', 'anc', 'cre', 'ubl', 'ch ', 'on the ', 'ity', 'cu', 'ir ', 'ph', 'Jap', 'more ', 'oug', 'New ', '? ', 'Ch', 'Ap', 'Cl', 'ack ', 'Bl', 'Com', 'ets ', 'market', 'prices ', 'us ', 'economy ', 'mer ', 'int', 'ke', 'elect', 'high', 'Frid', 'a b', ' m', 'Inc. ', '(AP', '(AP) ', '(AP) AP', '(AP) AP - ', 'lat', '.com', 'ales ', ', a ', 'fin', 'ed by ', 'about ', 'av', 'emp', 'EC ', 'Google ', 'publ', 'ck', 'company ', 'rad', 'rec', 'grow', 'bec', 'uch ', 'own ', 'ese ', 'ors ', 'Pl', '-m', 'inter', 'tr', 'entr', 'ros', 'quar', 'On', 'di', 'a s', 'earch', ' technolog', 'e and ', '..', 'ed the ', 'incl', 'includ', 'bus', 'busin', 'Rus', 'Russi', 'stom', 'ple ', 'Al', 'IT', 'IT ', 'tw', 'ers, ', 'ra', 'see', 'ing g', 'ment ', 'Oil ', 'look ', 'ock ']\n"
     ]
    }
   ],
   "source": [
    "print(len(char_tokenizer.vocabulary))\n",
    "print(char_tokenizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51169653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 125, 117, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer.encode(\"HelloØ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b88011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello<unk>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer.decode([36, 125, 117, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2fc9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "byt_tokenizer = BPEByteTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec903a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "byt_tokenizer.train(tokenizer_train, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76dd54f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "[b'<unk>', b'<pad>', b'<bos>', b'<eos>', b'\\x00', b'\\x01', b'\\x02', b'\\x03', b'\\x04', b'\\x05', b'\\x06', b'\\x07', b'\\x08', b'\\t', b'\\n', b'\\x0b', b'\\x0c', b'\\r', b'\\x0e', b'\\x0f', b'\\x10', b'\\x11', b'\\x12', b'\\x13', b'\\x14', b'\\x15', b'\\x16', b'\\x17', b'\\x18', b'\\x19', b'\\x1a', b'\\x1b', b'\\x1c', b'\\x1d', b'\\x1e', b'\\x1f', b' ', b'!', b'\"', b'#', b'$', b'%', b'&', b\"'\", b'(', b')', b'*', b'+', b',', b'-', b'.', b'/', b'0', b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', b'9', b':', b';', b'<', b'=', b'>', b'?', b'@', b'A', b'B', b'C', b'D', b'E', b'F', b'G', b'H', b'I', b'J', b'K', b'L', b'M', b'N', b'O', b'P', b'Q', b'R', b'S', b'T', b'U', b'V', b'W', b'X', b'Y', b'Z', b'[', b'\\\\', b']', b'^', b'_', b'`', b'a', b'b', b'c', b'd', b'e', b'f', b'g', b'h', b'i', b'j', b'k', b'l', b'm', b'n', b'o', b'p', b'q', b'r', b's', b't', b'u', b'v', b'w', b'x', b'y', b'z', b'{', b'|', b'}', b'~', b'\\x7f', b'\\x80', b'\\x81', b'\\x82', b'\\x83', b'\\x84', b'\\x85', b'\\x86', b'\\x87', b'\\x88', b'\\x89', b'\\x8a', b'\\x8b', b'\\x8c', b'\\x8d', b'\\x8e', b'\\x8f', b'\\x90', b'\\x91', b'\\x92', b'\\x93', b'\\x94', b'\\x95', b'\\x96', b'\\x97', b'\\x98', b'\\x99', b'\\x9a', b'\\x9b', b'\\x9c', b'\\x9d', b'\\x9e', b'\\x9f', b'\\xa0', b'\\xa1', b'\\xa2', b'\\xa3', b'\\xa4', b'\\xa5', b'\\xa6', b'\\xa7', b'\\xa8', b'\\xa9', b'\\xaa', b'\\xab', b'\\xac', b'\\xad', b'\\xae', b'\\xaf', b'\\xb0', b'\\xb1', b'\\xb2', b'\\xb3', b'\\xb4', b'\\xb5', b'\\xb6', b'\\xb7', b'\\xb8', b'\\xb9', b'\\xba', b'\\xbb', b'\\xbc', b'\\xbd', b'\\xbe', b'\\xbf', b'\\xc0', b'\\xc1', b'\\xc2', b'\\xc3', b'\\xc4', b'\\xc5', b'\\xc6', b'\\xc7', b'\\xc8', b'\\xc9', b'\\xca', b'\\xcb', b'\\xcc', b'\\xcd', b'\\xce', b'\\xcf', b'\\xd0', b'\\xd1', b'\\xd2', b'\\xd3', b'\\xd4', b'\\xd5', b'\\xd6', b'\\xd7', b'\\xd8', b'\\xd9', b'\\xda', b'\\xdb', b'\\xdc', b'\\xdd', b'\\xde', b'\\xdf', b'\\xe0', b'\\xe1', b'\\xe2', b'\\xe3', b'\\xe4', b'\\xe5', b'\\xe6', b'\\xe7', b'\\xe8', b'\\xe9', b'\\xea', b'\\xeb', b'\\xec', b'\\xed', b'\\xee', b'\\xef', b'\\xf0', b'\\xf1', b'\\xf2', b'\\xf3', b'\\xf4', b'\\xf5', b'\\xf6', b'\\xf7', b'\\xf8', b'\\xf9', b'\\xfa', b'\\xfb', b'\\xfc', b'\\xfd', b'\\xfe', b'\\xff', b'e ', b's ', b'in', b' t', b'er', b'an', b'd ', b't ', b'on', b' th', b'or', b'ar', b'y ', b'ing', b'en', b' the ', b', ', b'l ', b'es', b'at', b'o ', b'it', b'ro', b'a ', b'ter', b're', b'ic', b'ou', b'of', b'ing ', b'es ', b'om', b'al', b'th', b'st', b'lo', b'ed ', b'ec', b'ion', b'and ', b'il', b'. ', b' to ', b'el', b'ac', b'in ', b'ur', b'k ', b\"'s \", b'- ', b'for', b'et', b'al ', b'h ', b'er ', b'an ', b'is', b'of ', b'ha', b'uter', b'Re', b'ew', b'us', b'as', b'ear', b') ', b'Reuter', b'pl', b'ay', b'ed', b'id', b'on ', b'com', b'is ', b's, ', b'ut', b'ion ', b'n ', b'ig', b'ent', b'Th', b'at ', b'ow', b'm ', b'sh', b'are ', b'ap', b'for ', b'ai', b'ad', b'ex', b've ', b'ay ', b'In', b'uc', b'as ', b'pro', b'ol', b'un', b'en ', b'oo', b'pr', b'ew ', b'to ', b'p ', b'ers ', b'wh', b'ch', b'em', b'The ', b'ag', b'im', b'oc', b'um', b'the ', b'pan', b'(Reuter', b'(Reuters', b'(Reuters) ', b'os', b'ent ', b'ab', b'ect', b'ul', b'ir', b'si', b'ly ', b'ey ', b'ell', b'le ', b'con', b'in the ', b'ation', b'le', b'am', b'St', b'ver', b'ud', b'and', b'it ', b'wit', b's - ', b'ee', b'qu', b'day', b'A ', b'est', b'str', b'of the ', b's.', b'igh', b'compan', b'li', b'per', b'(Reuters) Reuter', b'(Reuters) Reuters - ', b'ark', b'wor', b'th ', b'enc', b'ld ', b'off', b'by ', b'oun', b'cl', b'ess ', b'br', b'ks ', b'ci', b'low', b'aid ', b'cor', b'AP', b'ter ', b'00', b'all ', b'ak', b'il ', b'econ', b'ies ', b'new ', b'ell ', b'est ', b'e s', b'ug', b' that ', b'ev', b'has ', b'ation ', b'all', b'mark', b'ts ', b'fro', b'ould ', b'op', b'ic ', b'bl', b'no', b'row', b'our', b'with ', b'bo', b'ore ', b'sp', b'out', b'ing the ', b'ip', b'af', b'ill', b'e, ', b'res', b'ad ', b': ', b'its ', b'over', b'et ', b'por', b'be', b' - ', b'ive ', b' b', b'or ', b'es, ', b'Goo', b'Goog', b'be ', b'ot', b'e.', b'ill ', b'vic', b'iz', b'loo', b'pric', b'econom', b's are ', b' the', b'from ', b'uct', b'ut ', b'ear ']\n"
     ]
    }
   ],
   "source": [
    "print(len(byt_tokenizer.vocabulary))\n",
    "print(byt_tokenizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fc9966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76, 303, 295, 199, 156]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byt_tokenizer.encode(\"HelloØ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "723f15d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HelloØ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byt_tokenizer.decode([76, 303, 295, 199, 156])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508dd8b8",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52206b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 128])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformer import TransformerEncoderModel\n",
    "\n",
    "model = TransformerEncoderModel(num_embeddings=len(char_tokenizer.vocabulary), d_model=128, padding_idx=0, nhead=8, dim_feedforward=4*128, num_layers=6)\n",
    "\n",
    "model([torch.tensor(char_tokenizer.encode(\"HelloØ there\")),torch.tensor(char_tokenizer.encode(\"HelloØ there, how are you?\"))]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
