{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283b93da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lasse\\Desktop\\kand_sem_1\\deep\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from bpe_tokenizer import BPECharacterTokenizer, BPEByteTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d41600",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"ag_news\")\n",
    "\n",
    "# create a tokenizer training dataset\n",
    "tokenizer_train = list(dataset[\"train\"][\"text\"][:100])\n",
    "\n",
    "char_tokenizer = BPECharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87e2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer.train(tokenizer_train, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b255c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "[' ', '!', '\"', '#', '$', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<unk>', '<pad>', '<bos>', '<eos>', 'e ', 's ', 'in', ' t', 'er', 'an', 'd ', 't ', 'on', ' th', 'or', 'ar', 'y ', 'ing', 'en', ' the ', ', ', 'l ', 'es', 'at', 'o ', 'it', 'ro', 'a ', 'ter', 're', 'ic', 'ou', 'of', 'ing ', 'es ', 'om', 'al', 'th', 'st', 'lo', 'ed ', 'ec', 'ion', 'and ', 'il', '. ', ' to ', 'el', 'ac', 'in ', 'ur', 'k ', \"'s \", '- ', 'for', 'et', 'al ', 'h ', 'er ', 'an ', 'is', 'of ', 'ha', 'uter', 'Re', 'ew', 'us', 'as', 'ear', ') ', 'Reuter', 'pl', 'ay', 'ed', 'id', 'on ', 'com', 'is ', 's, ', 'ut', 'ion ', 'n ', 'ig', 'ent', 'Th', 'at ', 'ow', 'm ', 'sh', 'are ', 'ap', 'for ', 'ai', 'ad', 'ex', 've ', 'ay ', 'In', 'uc', 'as ', 'pro', 'ol', 'un', 'en ', 'oo', 'pr', 'ew ', 'to ', 'p ', 'ers ', 'wh', 'ch', 'em', 'The ', 'ag', 'im', 'oc', 'um', 'the ', 'pan', '(Reuter', '(Reuters', '(Reuters) ', 'os', 'ent ', 'ab', 'ect', 'ul', 'ir', 'si', 'ly ', 'ey ', 'ell', 'le ', 'con', 'in the ', 'ation', 'le', 'am', 'St', 'ver', 'ud', 'and', 'it ', 'wit', 's - ', 'ee', 'qu', 'day', 'A ', 'est', 'str', 'of the ', 's.', 'igh', 'compan', 'li', 'per', '(Reuters) Reuter', '(Reuters) Reuters - ', 'ark', 'wor', 'th ', 'enc', 'ld ', 'off', 'by ', 'oun', 'cl', 'ess ', 'br', 'ks ', 'ci', 'low', 'aid ', 'cor', 'AP', 'ter ', '00', 'all ', 'ak', 'il ', 'econ', 'ies ', 'new ', 'ell ', 'est ', 'e s', 'ug', ' that ', 'ev', 'has ', 'ation ', 'all', 'mark', 'ts ', 'fro', 'ould ', 'op', 'ic ', 'bl', 'no', 'row', 'our', 'with ', 'bo', 'ore ', 'sp', 'out', 'ing the ', 'ip', 'af', 'ill', 'e, ', 'res', 'ad ', ': ', 'its ', 'over', 'et ', 'por', 'be', ' - ', 'ive ', ' b', 'or ', 'es, ', 'Goo', 'Goog', 'be ', 'ot', 'e.', 'ill ', 'vic', 'iz', 'loo', 'pric', 'econom', 's are ', ' the', 'from ', 'uct', 'ut ', 'ear ', 'ast ', 'ing a ', 'log', 'ing to ', 'we', 'e of ', 'ech', 'ity ', 'ong', 'to', 'ate ', 'up', 'ef', 'cr', 'ang', 'mon', 'rel', 'Inc', 'und', 'plan', 'des', 'out ', ' s', '-- ', 'ere ', '\\\\$', 'arg', 'up ', 'at the ', 'gr', 'ervic', 'ind', 'ult', 'yn', 'aga', 'ace ', 'iv', 'fir', 'par', \"' \", 'show', 'ial ', 'said ', 'pos', 'Fr', 'ail ', ', the ', 'dis', 'ed to ', 'ail', '  ', 'C ', 'can ', 's S', 'ian', \"y's \", 'our ', 'ack', 'one ', 'ile ', 'ob', 'ist', 'ome ', 'echno', 'echnolog', 'proj', '. B', 'aw', 'ard ', 'mer', 'oth', 'exp', ' to', 'wee', 'ine ', 'oil ', 'US', 's and ', 'for the ', 'sur', 'ov', ', and ', 'his ', 'anc', 'cre', 'ubl', 'ch ', 'on the ', 'ity', 'cu', 'ir ', 'ph', 'Jap', 'more ', 'oug', 'New ', '? ', 'Ch', 'Ap', 'Cl', 'ack ', 'Bl', 'Com', 'ets ', 'market', 'prices ', 'us ', 'economy ', 'mer ', 'int', 'ke', 'elect', 'high', 'Frid', 'a b', ' m', 'Inc. ', '(AP', '(AP) ', '(AP) AP', '(AP) AP - ', 'lat', '.com', 'ales ', ', a ', 'fin', 'ed by ', 'about ', 'av', 'emp', 'EC ', 'Google ', 'publ', 'ck', 'company ', 'rad', 'rec', 'grow', 'bec', 'uch ', 'own ', 'ese ', 'ors ', 'Pl', '-m', 'inter', 'tr', 'entr', 'ros', 'quar', 'On', 'di', 'a s', 'earch', ' technolog', 'e and ', '..', 'ed the ', 'incl', 'includ', 'bus', 'busin', 'Rus', 'Russi', 'stom', 'ple ', 'Al', 'IT', 'IT ', 'tw', 'ers, ', 'ra', 'see', 'ing g', 'ment ', 'Oil ', 'look ', 'ock ']\n"
     ]
    }
   ],
   "source": [
    "print(len(char_tokenizer.vocabulary))\n",
    "print(char_tokenizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51169653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 125, 117, 78]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer.encode(\"Hello√ò\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b88011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello<unk>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer.decode([32, 125, 117, 78])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1cb0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ail '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer.vocabulary[367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2fc9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "byt_tokenizer = BPEByteTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec903a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "byt_tokenizer.train(tokenizer_train, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76dd54f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "[b'\\x00', b'\\x01', b'\\x02', b'\\x03', b'\\x04', b'\\x05', b'\\x06', b'\\x07', b'\\x08', b'\\t', b'\\n', b'\\x0b', b'\\x0c', b'\\r', b'\\x0e', b'\\x0f', b'\\x10', b'\\x11', b'\\x12', b'\\x13', b'\\x14', b'\\x15', b'\\x16', b'\\x17', b'\\x18', b'\\x19', b'\\x1a', b'\\x1b', b'\\x1c', b'\\x1d', b'\\x1e', b'\\x1f', b' ', b'!', b'\"', b'#', b'$', b'%', b'&', b\"'\", b'(', b')', b'*', b'+', b',', b'-', b'.', b'/', b'0', b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', b'9', b':', b';', b'<', b'=', b'>', b'?', b'@', b'A', b'B', b'C', b'D', b'E', b'F', b'G', b'H', b'I', b'J', b'K', b'L', b'M', b'N', b'O', b'P', b'Q', b'R', b'S', b'T', b'U', b'V', b'W', b'X', b'Y', b'Z', b'[', b'\\\\', b']', b'^', b'_', b'`', b'a', b'b', b'c', b'd', b'e', b'f', b'g', b'h', b'i', b'j', b'k', b'l', b'm', b'n', b'o', b'p', b'q', b'r', b's', b't', b'u', b'v', b'w', b'x', b'y', b'z', b'{', b'|', b'}', b'~', b'\\x7f', b'\\x80', b'\\x81', b'\\x82', b'\\x83', b'\\x84', b'\\x85', b'\\x86', b'\\x87', b'\\x88', b'\\x89', b'\\x8a', b'\\x8b', b'\\x8c', b'\\x8d', b'\\x8e', b'\\x8f', b'\\x90', b'\\x91', b'\\x92', b'\\x93', b'\\x94', b'\\x95', b'\\x96', b'\\x97', b'\\x98', b'\\x99', b'\\x9a', b'\\x9b', b'\\x9c', b'\\x9d', b'\\x9e', b'\\x9f', b'\\xa0', b'\\xa1', b'\\xa2', b'\\xa3', b'\\xa4', b'\\xa5', b'\\xa6', b'\\xa7', b'\\xa8', b'\\xa9', b'\\xaa', b'\\xab', b'\\xac', b'\\xad', b'\\xae', b'\\xaf', b'\\xb0', b'\\xb1', b'\\xb2', b'\\xb3', b'\\xb4', b'\\xb5', b'\\xb6', b'\\xb7', b'\\xb8', b'\\xb9', b'\\xba', b'\\xbb', b'\\xbc', b'\\xbd', b'\\xbe', b'\\xbf', b'\\xc0', b'\\xc1', b'\\xc2', b'\\xc3', b'\\xc4', b'\\xc5', b'\\xc6', b'\\xc7', b'\\xc8', b'\\xc9', b'\\xca', b'\\xcb', b'\\xcc', b'\\xcd', b'\\xce', b'\\xcf', b'\\xd0', b'\\xd1', b'\\xd2', b'\\xd3', b'\\xd4', b'\\xd5', b'\\xd6', b'\\xd7', b'\\xd8', b'\\xd9', b'\\xda', b'\\xdb', b'\\xdc', b'\\xdd', b'\\xde', b'\\xdf', b'\\xe0', b'\\xe1', b'\\xe2', b'\\xe3', b'\\xe4', b'\\xe5', b'\\xe6', b'\\xe7', b'\\xe8', b'\\xe9', b'\\xea', b'\\xeb', b'\\xec', b'\\xed', b'\\xee', b'\\xef', b'\\xf0', b'\\xf1', b'\\xf2', b'\\xf3', b'\\xf4', b'\\xf5', b'\\xf6', b'\\xf7', b'\\xf8', b'\\xf9', b'\\xfa', b'\\xfb', b'\\xfc', b'\\xfd', b'\\xfe', b'\\xff', b'<unk>', b'<pad>', b'<bos>', b'<eos>', 133, 147, 215, 148, 215, 207, 132, 148, 221, 252, 225, 211, 153, 318, 211, 385, 76, 140, 216, 247, 213, 143, 221, 225, 129, 233, 215, 204, 213, 350, 248, 220, 205, 231, 220, 219, 233, 200, 228, 326, 339, 213, 78, 291, 209, 327, 196, 231, 139, 217, 186, 136, 77, 362, 237, 239, 327, 312, 330, 220, 245, 334, 220, 252, 232, 201, 73, 337, 353, 416, 212, 220, 218, 201, 205, 253, 252, 539, 245, 331, 142, 208, 188, 344, 230, 141, 219, 319, 221, 211, 359, 314, 251, 202, 250, 244, 209, 183, 359, 216, 191, 598, 219, 227, 222, 332, 320, 259, 144, 223, 226, 203, 210, 321, 243, 214, 210, 600, 226, 319, 456, 786, 859, 200, 226, 316, 197, 225, 219, 345, 261, 348, 254, 358, 317, 241, 209, 220, 206, 346, 217, 307, 229, 324, 340, 327, 202, 230, 318, 253, 97, 195, 344, 310, 344, 312, 638, 213, 315, 199, 1275, 1637, 1714, 317, 321, 318, 344, 240, 364, 251, 338, 363, 328, 219, 254, 333, 245, 441, 161, 353, 331, 145, 96, 212, 345, 204, 427, 362, 338, 349, 220, 358, 315, 342, 571, 313, 368, 468, 223, 236, 311, 206, 427, 221, 325, 497, 342, 229, 476, 314, 209, 227, 459, 297, 348, 345, 491, 703, 263, 109, 323, 321, 177, 330, 320, 249, 267, 217, 356, 130, 316, 609, 257, 292, 293, 396, 231, 227, 147, 353]\n"
     ]
    }
   ],
   "source": [
    "print(len(byt_tokenizer.vocabulary))\n",
    "print(byt_tokenizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fc9966",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<unk>'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbyt_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lasse\\Desktop\\kand_sem_1\\deep\\project\\bpe_tokenizer.py:36\u001b[39m, in \u001b[36mBPETokenizer.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     33\u001b[39m             i += \u001b[32m1\u001b[39m\n\u001b[32m     34\u001b[39m     tokens = new_tokens\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m unk_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_to_id_map\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<unk>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     37\u001b[39m ids = [\u001b[38;5;28mself\u001b[39m.token_to_id_map.get(t, unk_id) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: '<unk>'"
     ]
    }
   ],
   "source": [
    "byt_tokenizer.encode(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10794e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "text = [t for t in \"To display the results more clearly, we can iterate over the dictionary items (which return key-value pairs) and create a value-key list instead. This allows us to call sort() on it, since Python defaults to sorting by the first element in tuples. Using reverse=True produces descending order. The results show that the pair (101, 32) occurs most frequently, appearing 20 times. Searching for all occurrences of 101, 32 in the token list confirms these 20 instances.\"]\n",
    "\n",
    "alphabet = [\"<UNK>\"] + list(set(text))\n",
    "\n",
    "def merge(text, pair):\n",
    "    i=0\n",
    "    while i < len(text)-1:\n",
    "        if text[i] == pair[0] and text[i+1] == pair[1]:\n",
    "            del text[i+1]\n",
    "            text[i] = pair[0]+pair[1]\n",
    "        i += 1\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def byte_pair(text, alphabet, vocab_size):\n",
    "    pair_freq = defaultdict(int)\n",
    "\n",
    "    while len(alphabet) < vocab_size:\n",
    "\n",
    "        for a,b in zip(text[:-1], text[1:]):\n",
    "            pair_freq[(a,b)] += 1\n",
    "\n",
    "        pair = max(pair_freq, key=pair_freq.get)\n",
    "        text = merge(text, pair)\n",
    "        alphabet.append(pair[0]+pair[1])\n",
    "\n",
    "    return alphabet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f4abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = byte_pair(text, alphabet, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5e52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_future(text, i, n, crnt_token_id=0):\n",
    "    \n",
    "    if text[i:i+n] in alphabet and i+n <= len(text):\n",
    "        crnt_token_id = alphabet.index(text[i:i+n])\n",
    "        return check_future(text, i, n+1, crnt_token_id)\n",
    "    else:\n",
    "\n",
    "        return crnt_token_id, n-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cfb189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_tokenizer(text):\n",
    "    tokenized_text = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        token_id,n = check_future(text, i, 1)\n",
    "        tokenized_text.append(token_id)\n",
    "        i += n or 1\n",
    "    return tokenized_text\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d91272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 13, 13, 4, 0, 36, 0, 13]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer(\"hjeehjrje\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
