{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ee607fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "ag_news_dataset = datasets.load_dataset(\"ag_news\", split=\"test\")\n",
    "dataset_corrupted_letters = datasets.load_from_disk(os.path.join(\"ag_news_variations\", \"ag_news_corrupted_letters_test\"))\n",
    "dataset_corrupted = datasets.load_from_disk(os.path.join(\"ag_news_variations\", \"ag_news_corrupted_test\"))\n",
    "dataset_danish = datasets.load_from_disk(os.path.join(\"ag_news_variations\", \"ag_news_translated_da_test\"))\n",
    "dataset_icelandic = datasets.load_from_disk(os.path.join(\"ag_news_variations\", \"ag_news_translated_is_test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a31e281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer_path = \"tokenizers\"\n",
    "\n",
    "char_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=os.path.join(tokenizer_path, \"char_tokenizer.json\"),\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "\n",
    "byte_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=os.path.join(tokenizer_path, \"byte_tokenizer.json\"),\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "\n",
    "raw_byte_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=os.path.join(tokenizer_path, \"raw_byte_tokenizer.json\"),\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71c5e5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dataset_original, Tokenizer: char_tokenizer, Avg length: 50.331315789473685, OOV token ratio: 0.0\n",
      "Dataset: dataset_original, Tokenizer: byte_tokenizer, Avg length: 57.45828947368421, OOV token ratio: 0.0\n",
      "Dataset: dataset_original, Tokenizer: raw_byte_tokenizer, Avg length: 235.2992105263158, OOV token ratio: 0.0\n",
      "Dataset: dataset_corrupted_letters, Tokenizer: char_tokenizer, Avg length: 69.97105263157894, OOV token ratio: 0.0\n",
      "Dataset: dataset_corrupted_letters, Tokenizer: byte_tokenizer, Avg length: 75.14868421052631, OOV token ratio: 0.0\n",
      "Dataset: dataset_corrupted_letters, Tokenizer: raw_byte_tokenizer, Avg length: 235.2992105263158, OOV token ratio: 0.0\n",
      "Dataset: dataset_corrupted, Tokenizer: char_tokenizer, Avg length: 72.98210526315789, OOV token ratio: 0.10995124976562387\n",
      "Dataset: dataset_corrupted, Tokenizer: byte_tokenizer, Avg length: 84.35315789473684, OOV token ratio: 0.0\n",
      "Dataset: dataset_corrupted, Tokenizer: raw_byte_tokenizer, Avg length: 241.20986842105262, OOV token ratio: 0.0\n",
      "Dataset: dataset_danish, Tokenizer: char_tokenizer, Avg length: 96.04618421052632, OOV token ratio: 0.044392020834275174\n",
      "Dataset: dataset_danish, Tokenizer: byte_tokenizer, Avg length: 101.50736842105263, OOV token ratio: 0.0\n",
      "Dataset: dataset_danish, Tokenizer: raw_byte_tokenizer, Avg length: 244.36671052631579, OOV token ratio: 0.0\n",
      "Dataset: dataset_icelandic, Tokenizer: char_tokenizer, Avg length: 115.84421052631579, OOV token ratio: 0.16141460400537927\n",
      "Dataset: dataset_icelandic, Tokenizer: byte_tokenizer, Avg length: 134.22486842105263, OOV token ratio: 0.0\n",
      "Dataset: dataset_icelandic, Tokenizer: raw_byte_tokenizer, Avg length: 237.1967105263158, OOV token ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# tokenize dataset\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"])\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    return tokenized_dataset\n",
    "\n",
    "for dataset in [\n",
    "    (\"dataset_original\", ag_news_dataset),\n",
    "    (\"dataset_corrupted_letters\", dataset_corrupted_letters),\n",
    "    (\"dataset_corrupted\", dataset_corrupted),\n",
    "    (\"dataset_danish\", dataset_danish),\n",
    "    (\"dataset_icelandic\", dataset_icelandic),\n",
    "]:\n",
    "    for tokenizer in [(\"char_tokenizer\", char_tokenizer), (\"byte_tokenizer\", byte_tokenizer), (\"raw_byte_tokenizer\", raw_byte_tokenizer)]:\n",
    "        tokenized_dataset = tokenize_dataset(dataset[1], tokenizer[1])\n",
    "        # print some stats\n",
    "        print(\n",
    "            f\"Dataset: {dataset[0]}, Tokenizer: {tokenizer[0]}, Avg length: {sum(len(x) for x in tokenized_dataset['input_ids']) / len(tokenized_dataset)}, OOV token ratio: {sum(x.count(tokenizer[1].unk_token_id) for x in tokenized_dataset['input_ids']) / sum(len(x) for x in tokenized_dataset['input_ids'])}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
